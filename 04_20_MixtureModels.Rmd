---
title: "Mixture Models"
author: "Matthew Reimherr"
output:
  pdf_document: default
  html_notebook: default
  html_document: default
  word_document: default
header_includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
- \usepackage{amsthm}
math_includes:
- \DeclareMathOperator*{\argmax}{arg\,max}
---

\newcommand{\E}{\mathop{\mathrm{E}}}
\newcommand{\Cov}{\mathop{\mathrm{Cov}}}
\newcommand{\Var}{\mathop{\mathrm{Var}}}


\newcommand{\mcF}{{\mathcal{F}}}
\newcommand{\bH}{{\bf H}}
\newcommand{\mbR}{{\mathbb{R}}}
\newcommand{\bx}{{\bf{x}}}
\newcommand{\by}{{\bf y}}
\newcommand{\bY}{{\bf Y}}


\newcommand{\vep}{{\varepsilon}}


## Mixture Models and Clustering

The model we aim to fit is given by
\[
f(x) = \sum_{k=1}^K p_k f_k(x).
\]
Phrasing this in terms latent variables we have
\[
P(Z_i = k) = p_k \qquad X_i | Z_i = k \sim f_k.
\]
We will assume that $f_k \sim N(\mu_k, \sigma^2_k)$.  Let's do a bit of simulation first.

```{r}
set.seed(440)
mu<-c(1,3)
sig2<-c(1/2,1/2)
N<-100
p<-c(3/4,1/4)
n1<-floor(N*p[1])
n2<-N-n1

X1<-rnorm(n1,mean=mu[1],sd=sqrt(sig2[1]))
X2<-rnorm(n2,mean=mu[2],sd=sqrt(sig2[2]))

X<-c(X1,X2)
Z<-c(rep(1,n1),rep(2,n2))

hist(c(X1,X2))
plot(density(c(X1,X2)))

stripchart(X1,col="blue",xlim=c(min(X),max(X)))
stripchart(X2,col="red",add=TRUE)
```

We will now set up a Bayesian approach to fitting this model.  
\begin{align*}
& X_i | \{Z_i, \mu_k, \sigma^2_k, p_k\} \sim N(\mu_{Z_i}, \sigma^2_{Z_i}) \\
& Z_i | \{\mu_k, \sigma^2_k, p_k\} \sim Mult(p) \\
& \mu_k \sim N(\mu_0, \sigma^2_0) \\
& \sigma^2_k \sim 1/\sigma^2_k \\
& p \sim Dir(K,\alpha)
\end{align*}
The priors we assume to be independent.  The hyper parameters are $\mu_0,\sigma^2_0$ and $\alpha$.  We can actually work things out to allow $K$ to also be random using what's called a Dirichlet process (but we won't do that).  We want to work out a Gibbs sampler for this problem and exploit conjugate priors as we can.  Notice that the updates for $\mu_k$, $\sigma^k$ and $p$ are all pretty straight forward (the $p$ update is similar to the Beta case) as these are all conjugate priors (conditioned on everything else).  The last thing we need is to be able to draw the $Z_i$ conditioned on everything else.  Let me call the other parameters collectively $\theta$. Then
\[
P(Z_i =j | X_i,\theta)
= \frac{p(Z_i =j , X_i | \theta)}{p(X_i | \theta)}
= \frac{p( X_i | \theta,Z_=j) p(Z_i=j |\theta)}{\sum_k P(X_i | Z_i=k,\theta) p(Z_i = k|\theta)}
= \frac{p_{j}\varphi(X_i,\mu_j,\sigma^2_j)}{\sum_{k} p_k \varphi(X_i,\mu_k,\sigma^k)}
\]
where $\phi$ is a normal density. So, to draw $Z_i$, we need only compute the above ratios.  Let's give it a try.

One issue you will run into is that it is possible for the group labels to actuall switch, since they are arbitrary. One way to fix this is that you can force the means to be ascending.

```{r}
library(invgamma)
n_mc<-10000

mu_mc<-matrix(nrow=n_mc,ncol=2)
sig2_mc<-matrix(nrow=n_mc,ncol=2)
p_mc<-matrix(nrow=n_mc,ncol=2) # 2nd column is redundant
Z_mc<-matrix(nrow=n_mc,ncol=N)

# Hyper parameters
# You have to be careful with thise
# You don't want the variances to blow.
mu0<-0
sig20<-10
a_ig<-1
b_ig<-1
alpha_0<-c(1,1) # uniform

# Starting Values
mu_c<-c(0,1)
sig2_c<-c(1,1)
p_c<-c(1/2,1/2)
Z_c<-sample(c(1,2),N,replace=TRUE)
X1_c<-X[Z_c==1]
n1_c<-sum(Z_c==1)
X2_c<-X[Z_c==2]
n2_c<-sum(Z_c==2)

mu_mc[1,]<-mu_c
sig2_mc[1,]<-sig2_c
p_mc[1,]<-p_c
Z_mc[1,]<-Z_c

# Constraints
# To help with stability, you can add in 
# constraints to ensure your estimates don't blow up
var_con<-10


for(i in 2:n_mc){
  # Mean Updates
  var1_g<-1/(1/sig20 + n1_c/sig2_c[1])
  mean1_g<-(mu0/sig20 + sum(X1_c)/sig2_c[1])*var1_g
  
  var2_g<-1/(1/sig20 + n2_c/sig2_c[2])
  mean2_g<-(mu0/sig20 + sum(X2_c)/sig2_c[2])*var2_g
  
  # We can reject proposals where the means arent ordered properly
  # Alterntively, we can relabel at the end of each run.
  mean_prop<-rnorm(2,c(mean1_g,mean2_g),c(sqrt(var1_g),sqrt(var2_g)))
  
  #while(mean_prop[1]>mean_prop[2]){mean_prop<-rnorm(2,c(mean1_g,mean2_g),c(sqrt(var1_g),sqrt(var2_g)))}
  #mu_c<-mean_prop
  if(mean_prop[1]<mean_prop[2]){mu_c<-mean_prop}
  
  # Var Updates
  a1_ig<-a_ig+n1_c/2
  b1_ig<-b_ig+sum((X1_c-mu_c[1])^2)/2
  
  a2_ig<-a_ig+n2_c/2
  b2_ig<-b_ig+sum((X2_c-mu_c[2])^2)/2
  
  sig2_prop<-rinvgamma(2,shape=c(a1_ig,a2_ig),rate=c(b1_ig,b2_ig)) # Note difference with wiki
  if(max(sig2_prop)<var_con){sig2_c<-sig2_prop}
  
  # p Updates
  # Note the conjugate prior for multinomial is dirichlet
  # With two categories, this is just beta
  # Simple simulation method is to use exponentials/gammas
  alpha_g <- alpha_0+summary(as.factor(Z_c)) 
  E<-rexp(2,rate=1/alpha_g)
  p_c<-E/sum(E)
  
  # Z updates
  f1<-dnorm(X,mean=mu_c[1],sd=sqrt(sig2_c[1]))
  f2<-dnorm(X,mean=mu_c[2],sd=sqrt(sig2_c[2]))
  probs<-cbind(f1*p_c[1]/(f1*p_c[1]+f2*p_c[2]),f2*p_c[2]/(f1*p_c[1]+f2*p_c[2]))
  
  myfun<-function(x){sample(c(1,2),1,prob=x)}
  Z_c<-apply(probs,1,FUN=myfun)
  n1_c<-sum(Z_c==1)
  n2_c<-sum(Z_c==2)
  X1_c<-X[Z_c==1]
  X2_c<-X[Z_c==2]
  
  # Gather updates
  mu_mc[i,]<-mu_c
  sig2_mc[i,]<-sig2_c
  p_mc[i,]<-p_c
  Z_mc[i,]<-Z_c
}
```

Let's look at the plots.
```{r}
plot(mu_mc[,1])
plot(mu_mc[-(1:2000),1])
plot(density(mu_mc[-(1:2000),1]))
abline(v=mu[1])
```
```{r}
plot(mu_mc[,2])
plot(density(mu_mc[-(1:2000),2]))
abline(v=mu[2])
```
```{r}
plot(sig2_mc[,1])
plot(sig2_mc[-(1:2000),1])
plot(density(sig2_mc[-(1:2000),1]))
abline(v=sig2[1])
```
```{r}
plot(sig2_mc[,2])
plot(sig2_mc[-(1:2000),2])
plot(density(sig2_mc[-(1:2000),2]))
abline(v=sig2[2])
```

```{r}
plot(p_mc[,1])
plot(p_mc[-(1:2000),1])
plot(density(p_mc[-(1:2000),1]))
abline(v=p[1])
```

We can even use the matrix of $Z$s to form predictions of which observations go with which group (though we can also use the parameter estimates).
```{r}
Zave<-colMeans(Z_mc[-(1:2000),])
Zhat<-rep(1,times=N)
Zhat[Zave>1.5]=2


# Confusion Matrix
table(Zhat,Z)

# Error Rate
P12<-sum(Zhat==1 & Z==2)
P21<-sum(Zhat==2& Z==1)

(P12+P21)/N

```


Let's compare with frequentist clustering.
```{r}
#install.packages("mclust")
library("mclust")
myfit<-Mclust(X,G=2,modelNames="V")
mytable<-table(myfit$classification,Z)

mytable
(mytable[1,2]+mytable[2,1])/N
```





























